{
  "slug": "ab-testing",
  "title": "A/B Testing",
  "duration": "2-4 weeks",
  "participants": "1000+ users",
  "complexity": "High",
  "purpose": "Compare two versions of a design or feature to determine which performs better based on real user behaviour and statistical evidence.",
  "tools": [
    {
      "name": "Optimal Workshop",
      "description": "Running preference tests",
      "icon": "fa-chart-bar",
      "link": "https://app.optimalworkshop.com/"
    },
    {
      "name": "Google Analytics",
      "description": "Live environment testing",
      "icon": "fa-vial"
    },
    {
      "name": "Figma Slides",
      "description": "Insights Deck (Max 3 per piece of research)",
      "icon": "fa-presentation",
      "link": "https://www.figma.com/slides/RodMmoG77vJztZsM6Z3Ean/2025-Research-Playback?node-id=484-8400&t=2tZP1VJliVSeW8i6-1"
    }
  ],
  "steps": [
    {
      "title": "üß≠ Identify Your Test Type",
      "description": "Not all A/B tests are created equal. Start by identifying what kind of test you're running to choose the right approach.\n\nDesign Preference Tests\n- Testing visual elements (colors, layouts, copy)\n- Pre-launch validation\n- Quick turnaround (days)\n- Smaller sample sizes OK\n\nBehavioural Impact Tests\n- Testing functionality changes\n- Post-launch optimisation\n- Longer duration (weeks)\n- Need larger sample sizes\n\nQuick Decision Guide:\n- Pre-launch + visual changes = Preference Test\n- Live product + behaviour metrics = Live A/B Test\n- Testing copy or messaging = Either (depends on impact)\n- Major UX changes = Live A/B Test\n\nComplexity Check:\n- Low: Single element change (button color, copy)\n- Medium: Multiple elements or flow changes\n- High: Full page redesigns or feature overhauls",
      "screenshot": "",
      "screenshots": []
    },
    {
      "title": "üß™ Choose Your Testing Method",
      "description": "Match your test type to the right method and tools.\n\nPreference Testing (Pre-Launch)\n- When to use: Design decisions, copy variations, layout options\n- Tool: Optimal Workshop\n- Duration: 2-3 days\n- Sample: 30-50 per variant\n- Pros: Fast, cheap, good for visual preferences\n- Cons: Shows preference, not actual behaviour\n\nLive A/B Testing (Post-Launch)\n- When to use: Feature changes, conversion optimisation, real behaviour\n- Tool: Google Analytics, your app's analytics\n- Duration: 2-4 weeks\n- Sample: 1000+ per variant\n- Pros: Real behaviour data, actual metrics\n- Cons: Takes longer, needs traffic, more complex\n\nHybrid Approach\nFor major changes, consider:\n1. Preference test first (validate direction)\n2. Live A/B test second (confirm behaviour)\n\nBanking-Specific Considerations:\n- Security features: Always live test\n- Payment flows: Live test with careful monitoring\n- Marketing copy: Preference test usually sufficient\n- New features: Start with preference, follow with live",
      "screenshot": "",
      "screenshots": []
    },
    {
      "title": "üìê Define Your Hypothesis",
      "description": "A clear hypothesis is the foundation of a good A/B test. Without it, you're just guessing.\n\nHypothesis Template:\n\"If we [make this change], then [this metric] will [increase/decrease] by [amount] because [reason based on user behaviour].\"\n\nStrong Hypothesis Examples:\n‚úÖ \"If we add a progress indicator to the loan application, then completion rates will increase by 15% because users will understand how many steps remain.\"\n\n‚úÖ \"If we change 'Submit' to 'Get My Rate' on the loan calculator, then click-through rates will increase by 10% because it's more specific and action-oriented.\"\n\nWeak Hypothesis Examples:\n‚ùå \"The new design will perform better\" (too vague)\n‚ùå \"Users will like the blue button more\" (not measurable)\n‚ùå \"This will reduce calls\" (no reasoning or amount)\n\nCommon Pitfalls:\n- Testing multiple things at once\n- No clear success metric\n- Hypothesis based on internal opinions, not user data\n- Unrealistic improvement expectations\n\nBanking Examples:\n- Self-service adoption: \"If we add quick balance to login page...\"\n- Security friction: \"If we reduce authentication steps...\"\n- Product discovery: \"If we personalize the offers shown...\"",
      "screenshot": "",
      "screenshots": []
    },
    {
      "title": "üß¨ Design Your Variants",
      "description": "Create meaningful, controlled variations that will give you clear insights.\n\nControl (Version A)\n- Your current design/experience\n- Don't change anything\n- This is your baseline for comparison\n\nVariant (Version B)\n- Change ONE thing based on your hypothesis\n- Make the change noticeable enough to impact behaviour\n- Keep everything else identical\n\nWhat to Change:\n‚úÖ Copy (headlines, CTAs, descriptions)\n‚úÖ Visual hierarchy (size, color, placement)\n‚úÖ Flow (steps, order, required fields)\n‚úÖ Features (add/remove functionality)\n\nWhat NOT to Change:\n‚ùå Multiple elements at once\n‚ùå Tiny tweaks users won't notice\n‚ùå Core functionality without backup plan\n‚ùå Security or compliance elements\n\nQuality Checklist:\n- [ ] Both versions work on all devices\n- [ ] Accessibility standards met (WCAG AA)\n- [ ] No broken functionality\n- [ ] Clear visual difference\n- [ ] Realistic content (no lorem ipsum)\n\nBanking-Specific:\n- Keep legal disclaimers consistent\n- Don't test security features\n- Consider vulnerable customers\n- Test with real monetary amounts",
      "screenshot": "",
      "screenshots": []
    },
    {
      "title": "üìä Calculate Sample Size",
      "description": "Ensure your test will have enough participants to detect meaningful differences.\n\nKey Inputs for Calculation:\n- Baseline conversion rate: Your current performance\n- Minimum detectable effect: Smallest change you care about (usually 5-10%)\n- Statistical confidence: 95% standard\n- Statistical power: 80% standard\n\nQuick Reference:\n- High traffic feature (10k+ users/week): Test anything\n- Medium traffic (1k-10k/week): 10%+ improvements detectable\n- Low traffic (<1k/week): Only major changes detectable\n\nSample Size Examples:\n- 5% baseline rate, detect 20% relative improvement: ~3,100 per variant\n- 20% baseline rate, detect 10% relative improvement: ~3,900 per variant\n- 50% baseline rate, detect 5% relative improvement: ~6,200 per variant\n\nLow Traffic Strategies:\n- Run test longer (but watch for seasonality)\n- Test bigger changes only\n- Use preference testing instead\n- Focus on high-traffic features\n- Consider sequential testing\n\nBanking Considerations:\n- Account for weekly/monthly patterns\n- Consider payday impacts\n- Business vs personal banking segments\n- New vs existing customer behaviour\n\n[Link to Sample Size Calculator]",
      "screenshot": "",
      "screenshots": []
    },
    {
      "title": "‚öôÔ∏è Set Up Your Test",
      "description": "Proper setup is critical - mistakes here invalidate your entire test.\n\nPreference Test Setup (Optimal Workshop):\n1. Upload both design variations\n2. Randomize presentation order\n3. Use consistent question format\n4. Include \"Why?\" follow-up questions\n5. Test the test yourself first\n\nLive A/B Test Setup:\n1. Technical Setup\n   - 50/50 traffic split (unless unequal)\n   - User stays in same variant throughout\n   - Proper event tracking configured\n   - Fallback for errors\n\n2. Quality Checks\n   - [ ] Both variants load correctly\n   - [ ] Tracking fires on all events\n   - [ ] Mobile and desktop work\n   - [ ] No console errors\n   - [ ] Page performance similar\n\n3. Pre-Launch Checklist\n   - [ ] Stakeholders informed\n   - [ ] Support team briefed\n   - [ ] Rollback plan ready\n   - [ ] Monitoring alerts set\n   - [ ] Test data excluded\n\nCommon Setup Errors:\n- Forgetting mobile users\n- Not excluding internal traffic\n- Inconsistent user assignment\n- Missing edge cases\n- No error handling\n\nBanking-Specific:\n- Exclude staff traffic\n- Check compliance approval\n- Monitor for security issues\n- Plan for high-value transactions",
      "screenshot": "",
      "screenshots": []
    },
    {
      "title": "üìà Monitor and Measure",
      "description": "Once live, resist the urge to peek at results every hour. Statistical significance takes time.\n\nWeek 1: Setup Verification\n- Confirm data flowing correctly\n- Check for technical issues\n- Verify 50/50 split working\n- Don't make decisions yet\n- Watch for major problems only\n\nWeek 2: Early Trends\n- Patterns starting to emerge\n- Still not significant\n- Check for segment differences\n- Monitor secondary metrics\n- Resist urge to call it\n\nWeek 3-4: Decision Time\n- Statistical significance reached\n- Patterns stable\n- Ready for analysis\n- Can make decision\n\nWhat to Measure:\n\nPrimary Metrics:\n- The one in your hypothesis\n- Must reach significance here\n\nSecondary Metrics:\n- Related behaviours\n- Downstream impacts\n- Potential negative effects\n\nSegments to Check:\n- New vs returning users\n- Mobile vs desktop\n- Demographics\n- Product holdings\n\nRed Flags to Watch:\n- Variant getting way more/less traffic\n- Metrics wildly different from baseline\n- Increased error rates\n- Customer complaints spike\n- Call centre volume changes\n\nWhen to Stop Early:\n- Technical issues\n- Significant negative impact\n- Compliance concerns raised",
      "screenshot": "",
      "screenshots": []
    },
    {
      "title": "üìä Analyse and Interpret Results",
      "description": "The test is done - now make sense of what happened.\n\nReading Results:\n\nStatistical Significance\n- Need 95% confidence minimum\n- p-value < 0.05\n- Don't cherry-pick segments that \"won\"\n\nPractical Significance\n- 2% improvement with significance = Real but small\n- Consider implementation effort\n- Factor in long-term impact\n\nResult Scenarios:\n\n‚úÖ Clear Winner (B beats A by 10%+)\n- Implement Version B\n- Document why it won\n- Apply learnings elsewhere\n\nü§∑ No Significant Difference\n- Both versions equally effective\n- Choose based on other factors\n- Still valuable learning\n\n‚ùå B Loses\n- Keep Version A\n- Understand why B failed\n- This is still success!\n\n‚ö†Ô∏è Mixed Results\n- B wins primary, loses secondary\n- Different segments prefer different\n- Need deeper analysis\n\nBanking Examples:\n\nConversion up, completion down:\n\"More users start but fewer finish - Version B attracts wrong users or creates false expectations\"\n\nDesktop wins, mobile loses:\n\"Design works on larger screens but mobile experience degraded - need mobile-specific solution\"\n\nNew users love it, existing hate it:\n\"Change disrupts learned behaviour - consider gradual rollout or user education\"\n\nWhat to do with segments:\n- Only trust pre-defined segments\n- Post-hoc analysis = dangerous\n- Consider personalization if clear segment preferences",
      "screenshot": "",
      "screenshots": []
    },
    {
      "title": "üìö Document and Share Learnings",
      "description": "Don't let valuable insights disappear - document them properly so the whole team learns.\n\nInsights Deck Structure:\n\n1. Executive Summary (1 slide)\n   - What we tested and why\n   - Key result in one sentence\n   - Recommendation\n\n2. Test Details (1 slide)\n   - Hypothesis\n   - Variants shown\n   - Timeline and sample size\n\n3. Results & Insights (1 slide)\n   - Primary metric impact\n   - Secondary effects\n   - Segment differences\n   - Why we think it happened\n\nWriting Effective Insights:\n\n‚úÖ Good: \"Customers are 15% more likely to complete transfers when we show the daily limit upfront, suggesting transparency reduces anxiety about hitting limits mid-transfer.\"\n\n‚ùå Bad: \"Version B had a 67% preference rate vs 33% for Version A with p<0.05.\"\n\n‚úÖ Good: \"Removing the security question on low-value transfers increased completion by 8% without increasing fraud, indicating our risk threshold was too conservative.\"\n\n‚ùå Bad: \"The new flow performed better.\"\n\nWhere to Store Results:\n- Design file annotations\n- Research repository/wiki\n- Component library notes\n- Team playback recording\n\nInclude:\n- Screenshots of both versions\n- Specific metrics and confidence\n- Implementation recommendations\n- Future test ideas\n- Lessons for other teams\n\nBanking-Specific Documentation:\n- Compliance considerations\n- Risk assessment results\n- Customer segment impacts\n- Support team feedback\n- Technical implementation notes",
      "screenshot": "",
      "screenshots": []
    }
  ]
}